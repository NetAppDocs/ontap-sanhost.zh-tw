---
sidebar: sidebar 
permalink: nvme_rhel_94.html 
keywords: nvme, linux, rhel, red hat, enterprise 
summary: 如何使用 ONTAP 設定適用於 RHEL 9.4 的 NVMe 主機 
---
= 適用於 ONTAP 的 RHEL 9.4 的 NVMe 主機組態
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Red Hat Enterprise Linux （ RHEL ） 9.4 搭配非對稱命名空間存取（ ANA ）、可支援 NVMe over Fabrics （ NVMe over Fabric 、 NVMe of ）、包括 NVMe over Fibre Channel （ NVMe / FC ）和其他傳輸。在 NVMe 環境中、 ANA 等同於 iSCSI 和 FC 環境中的 ALUA 多重路徑、並以核心內建 NVMe 多重路徑來實作。

下列支援適用於 ONTAP 的 RHEL 9.4 NVMe 主機組態：

* 支援 NVMe over TCP （ NVMe / TCP ）、以及 NVMe / FC 。原生的 NetApp 外掛程式 `nvme-cli` 套件會同時顯示 NVMe / FC 和 NVMe / TCP 命名空間的 ONTAP 詳細資料。
* 在指定主機匯流排介面卡（ HBA ）上的同一主機上使用 NVMe 和 SCSI 共存流量、而不使用明確的 dm-multipath 設定、以避免使用 NVMe 命名空間。


如需支援組態的詳細資訊、請參閱 link:https://mysupport.netapp.com/matrix/["NetApp 互通性對照表工具"^]。



== 功能

* 根據預設、 RHEL 9.4 已啟用 NVMe 命名空間的核心內建 NVMe 多重路徑、因此不需要明確的設定。
* 支援使用 NVMe / FC 傳輸協定的 SAN 開機。




== 已知限制

沒有已知的限制。



== 驗證軟體版本

您可以使用下列程序來驗證最低支援的 RHEL 9.4 軟體版本。

.步驟
. 在伺服器上安裝 RHEL 9.4 。安裝完成後、請確認您執行的是指定的 RHEL 9.4 核心：
+
[listing]
----
# uname -r
----
+
* 輸出範例： *

+
[listing]
----
5.14.0-423.el9.x86_64
----
. 安裝「NVMe-CLI（NVMe - CLI）套件：
+
[listing]
----
# rpm -qa|grep nvme-cli
----
+
* 輸出範例： *

+
[listing]
----
nvme-cli-2.6-4.el9.x86_64
----
. 安裝 `libnvme` 套件：
+
[listing]
----
#rpm -qa|grep libnvme
----
+
* 輸出範例 *

+
[listing]
----
libnvme-1.6-1.el9.x86_64
----
. 在 RHEL 9.4 主機上、檢查 hostnqn 字串 `/etc/nvme/hostnqn`：
+
[listing]
----
# cat /etc/nvme/hostnqn
----
+
* 輸出範例 *

+
[listing]
----
nqn.2014-08.org.nvmexpress:uuid: uuid:4c4c4544-0036-5610-804a-c7c04f365a32
----
. 確認 `hostnqn` 字串符合 `hostnqn` ONTAP 陣列上對應子系統的字串：
+
[listing]
----
::> vserver nvme subsystem host show -vserver vs_coexistence_LPE36002
----
+
* 輸出範例： *

+
[listing]
----
Vserver     Subsystem          Host NQN
----------- --------------- ----------------------------------------------------------
vs_coexistence_LPE36002   nvme    nqn.2014-08.org.nvmexpress:uuid: 4c4c4544-0036-5610-804a-
----
+

NOTE: 如果是 `hostnqn` 字串不相符、請使用 `vserver modify` 命令來更新 `hostnqn` 對應 ONTAP 陣列子系統上的字串、以符合 `hostnqn` 字串來源 `/etc/nvme/hostnqn` 在主機上。





== 設定NVMe/FC

您可以為 Broadcom / Emulex 或 Marvell/Qlogic 介面卡設定 NVMe / FC 。

[role="tabbed-block"]
====
.Broadcom / Emulex
--
.步驟
. 確認您使用的是支援的介面卡機型：
+
[listing]
----
# cat /sys/class/scsi_host/host*/modelname
----
+
* 輸出範例： *

+
[listing]
----
LPe36002-M64
LPe36002-M64

----
+
[listing]
----
# cat /sys/class/scsi_host/host*/modeldesc
----
+
* 輸出範例： *

+
[listing]
----
Emulex LightPulse LPe36002-M64 2-Port 64Gb Fibre Channel Adapter
Emulex LightPulse LPe36002-M64 2-Port 64Gb Fibre Channel Adapter
----
. 驗證您使用的是建議的Broadcom `lpfc` 韌體與收件匣驅動程式：
+
[listing]
----
# cat /sys/class/scsi_host/host*/fwrev
14.2.673.40, sli-4:6:d
14.2.673.40, sli-4:6:d


# cat /sys/module/lpfc/version
0:14.2.0.16
----
+
如需支援的介面卡驅動程式和韌體版本的最新清單、請參閱 link:https://mysupport.netapp.com/matrix/["NetApp 互通性對照表工具"^]。

. 請確認 `lpfc_enable_fc4_type` 設為 `3`：
+
[listing]
----
# cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
3
----
. 確認啟動器連接埠已啟動並正在執行、而且您可以看到目標生命體：
+
[listing]
----
# cat /sys/class/fc_host/host*/port_name
0x100000109b3c081f
0x100000109b3c0820

----
+
[listing]
----
# cat /sys/class/fc_host/host*/port_state
Online
Online
----
+
[listing, subs="+quotes"]
----
# cat /sys/class/scsi_host/host*/nvme_info
NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x100000109b3c081f WWNN x200000109b3c081f DID x062300 *ONLINE*
NVME RPORT       WWPN x2143d039ea165877 WWNN x2142d039ea165877 DID x061b15 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2145d039ea165877 WWNN x2142d039ea165877 DID x061115 *TARGET DISCSRVC ONLINE*
NVME Statistics
LS: Xmt 000000040b Cmpl 000000040b Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000001f5c4538 Issue 000000001f58da22 OutIO fffffffffffc94ea
abort 00000630 noxri 00000000 nondlp 00001071 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00000630 Err 0001bd4a
NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x100000109b3c0820 WWNN x200000109b3c0820 DID x062c00 *ONLINE*
NVME RPORT       WWPN x2144d039ea165877 WWNN x2142d039ea165877 DID x060215 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2146d039ea165877 WWNN x2142d039ea165877 DID x061815 *TARGET DISCSRVC ONLINE*
NVME Statistics
LS: Xmt 000000040b Cmpl 000000040b Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000001f5c3618 Issue 000000001f5967a4 OutIO fffffffffffd318c
abort 00000629 noxri 00000000 nondlp 0000044e qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00000629 Err 0001bd3d

----


--
.適用於 NVMe / FC 的 Marvell/QLogic FC 介面卡
--
.步驟
. RHEL 9.4 GA 核心中隨附的原生收件匣 qla2xxx 驅動程式具有 ONTAP 支援所需的最新修正。確認您執行的是支援的介面卡驅動程式和韌體版本：
+
[listing]
----
# cat /sys/class/fc_host/host*/symbolic_name
----
+
* 輸出範例 *

+
[listing]
----
QLE2872 FW:v9.12.01 DVR:v10.02.09.100-k
QLE2872 FW:v9.12.01 DVR:v10.02.09.100-k
----
. 請確認 `ql2xnvmeenable` 已設定。這可讓 Marvell 介面卡作為 NVMe / FC 啟動器運作：
+
[listing]
----
# cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
1
----


--
====


=== 啟用 1MB I/O （選用）

ONTAP 在識別控制器資料中報告的 MDTS （ MAX Data 傳輸大小）為 8 、表示最大 I/O 要求大小可達 1MB 。不過、若要針對 Broadcom NVMe / FC 主機發出大小為 1 MB 的 I/O 要求、您必須增加 `lpfc` 的價值 `lpfc_sg_seg_cnt` 從預設值 64 到 256 。

.步驟
. 將「lfc_sg_seg_cnt"參數設為256。
+
[listing]
----
# cat /etc/modprobe.d/lpfc.conf
options lpfc lpfc_sg_seg_cnt=256
----
. 執行「dracut -f」命令、然後重新啟動主機。
. 驗證「lfc_sg_seg_cnt"是否為256。
+
[listing]
----
# cat /sys/module/lpfc/parameters/lpfc_sg_seg_cnt
256
----



NOTE: 這不適用於 Qlogic NVMe / FC 主機。



== 設定NVMe/TCP

NVMe / TCP 沒有自動連線功能。因此、您必須手動執行 NVMe / TCP 連線或全部連線功能、才能探索 NVMe / TCP 子系統和命名空間。

您可以使用下列程序來設定 NVMe / TCP 。

.步驟
. 確認啟動器連接埠可在支援的NVMe/TCP LIF中擷取探索記錄頁面資料：
+
[listing]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
* 輸出範例： *

+
[listing, subs="+quotes"]
----
# nvme discover -t tcp -w 192.168.167.1 -a 192.168.167.16

Discovery Log Number of Records 8, Generation counter 10
=====Discovery Log Entry 0======
trtype:  tcp
adrfam:  ipv4
subtype: current discovery subsystem
treq:    not specified
portid:  11
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.983de7f4b39411ee871ed039ea954d18:
discovery
traddr:  192.168.167.8
eflags:  explicit discovery connections, duplicate discovery information
sectype: none
=====Discovery Log Entry 1======
trtype:  tcp
adrfam:  ipv4
subtype: current discovery subsystem
treq:    not specified
portid:  9
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.983de7f4b39411ee871ed039ea954d18:
discovery
traddr:  192.168.166.8
eflags:  explicit discovery connections, duplicate discovery information
sectype: none
=====Discovery Log Entry 2======
trtype:  tcp
adrfam:  ipv4
subtype: current discovery subsystem
treq:    not specified
portid:  12
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.983de7f4b39411ee871ed039ea954d18:
discovery
traddr:  192.168.167.7
eflags:  explicit discovery connections, duplicate discovery information
sectype: none
=====Discovery Log Entry 3======
trtype:  tcp
adrfam:  ipv4
subtype: current discovery subsystem
treq:    not specified
portid:  10
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.983de7f4b39411ee871ed039ea954d18:
discovery
traddr:  192.168.166.7
eflags:  explicit discovery connections, duplicate discovery information
sectype: none
=====Discovery Log Entry 4======
trtype:  tcp
adrfam:  ipv4
subtype: nvme subsystem
treq:    not specified
portid:  11
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.983de7f4b39411ee871ed039ea954d18:subsystem.nvme_tcp_1
traddr:  192.168.167.8
eflags:  none
sectype: none
=====Discovery Log Entry 5======
trtype:  tcp
adrfam:  ipv4
subtype: nvme subsystem
treq:    not specified
portid:  9
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.983de7f4b39411ee871ed039ea954d18:subsystem.nvme_tcp_1
traddr:  192.168.166.8
eflags:  none
sectype: none
=====Discovery Log Entry 6======
trtype:  tcp
adrfam:  ipv4
subtype: nvme subsystem
treq:    not specified
portid:  12
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.983de7f4b39411ee871ed039ea954d18:subsystem.nvme_tcp_1
traddr:  192.168.167.7
eflags:  none
sectype: none
=====Discovery Log Entry 7======
trtype:  tcp
adrfam:  ipv4
subtype: nvme subsystem
treq:    not specified
portid:  10
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:sn.983de7f4b39411ee871ed039ea954d18:subsystem.nvme_tcp_1
traddr:  192.168.166.7
eflags:  none
sectype: none
----
. 確認其他的 NVMe / TCP 啟動器目標 LIF 組合能夠成功擷取探索記錄頁面資料：
+
[listing]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
* 輸出範例： *

+
[listing]
----
#nvme discover -t tcp -w 192.168.166.6 -a 192.168.166.7
#nvme discover -t tcp -w 192.168.166.6 -a 192.168.166.8
#nvme discover -t tcp -w 192.168.167.6 -a 192.168.167.7
#nvme discover -t tcp -w 192.168.167.6 -a 192.168.167.8
----
. 執行 `nvme connect-all` 跨所有節點支援的 NVMe / TCP 啟動器目標生命體執行命令：
+
[listing]
----
nvme connect-all -t tcp -w host-traddr -a traddr
----
+
* 輸出範例： *

+
[listing]
----
#	nvme	connect-all	-t	tcp	-w	192.168.166.6	-a	192.168.166.7
#	nvme	connect-all	-t	tcp	-w	192.168.166.6	-a	192.168.166.8
#	nvme	connect-all	-t	tcp	-w	192.168.167.6	-a	192.168.167.7
#	nvme	connect-all	-t	tcp	-w	192.168.167.6	-a	192.168.167.8
----



NOTE: 從 RHEL 9.4 開始、是 NVMe / TCP 的預設設定 `ctrl_loss_tmo` 逾時已關閉、這表示重試次數沒有限制（無限期重試）。因此、您不需要手動設定特定的 `ctrl_loss_tmo` 使用時的逾時時間 `nvme connect` 或 `nvme connect-all` 命令（選項 -l ）。在這種預設行為下、當路徑故障時、 NVMe / TCP 控制器不會發生逾時、而且會無限期保持連線。



== 驗證NVMe

您可以使用下列程序來驗證 NVMe 。

.步驟
. 確認已啟用核心內建 NVMe 多重路徑：
+
[listing]
----
# cat /sys/module/nvme_core/parameters/multipath
Y
----
. 驗證個別 ONTAP 命名空間的適當 NVMe 設定（例如、模型設定為 NetApp ONTAP 控制器、負載平衡 iopolicing 設定為循環）是否正確反映在主機上：
+
[listing]
----
# cat /sys/class/nvme-subsystem/nvme-subsys*/model
NetApp ONTAP Controller
NetApp ONTAP Controller
----
+
[listing]
----
# cat /sys/class/nvme-subsystem/nvme-subsys*/iopolicy
round-robin
round-robin
----
. 確認已在主機上建立並正確探索命名空間：
+
[listing]
----
# nvme list
----
+
* 輸出範例： *

+
[listing]
----
Node         SN                   Model
---------------------------------------------------------
/dev/nvme4n1 81Ix2BVuekWcAAAAAAAB	NetApp ONTAP Controller


Namespace Usage    Format             FW             Rev
-----------------------------------------------------------
1                 21.47 GB / 21.47 GB	4 KiB + 0 B   FFFFFFFF
----
. 確認每個路徑的控制器狀態均為有效、且具有正確的ANA狀態：
+
[role="tabbed-block"]
====
.NVMe / FC
--
[listing]
----
# nvme list-subsys /dev/nvme5n21
----
* 輸出範例： *

[listing, subs="+quotes"]
----
nvme-subsys4 - NQN=nqn.1992-08.com.netapp:sn.efd7989cb10111ee871ed039ea954d18:subsystem.nvme
            hostnqn=nqn.2014-08.org.nvmexpress:uuid:d3b581b4-c975-11e6-8425-0894ef31a074
 iopolicy=round-robin
 \
  +- nvme2 fc traddr=nn-0x2013d039ea951c45:pn-0x2018d039ea951c45,host_traddr=nn-0x200000109bdacc76:pn-0x100000109bdacc76 live *non-optimized*
  +- nvme3 fc traddr=nn-0x2013d039ea951c45:pn-0x2017d039ea951c45,host_traddr=nn-0x200000109bdacc75:pn-0x100000109bdacc75 live *non-optimized*
  +- nvme5 fc traddr=nn-0x2013d039ea951c45:pn-0x2016d039ea951c45,host_traddr=nn-   0x200000109bdacc76:pn-0x100000109bdacc76 live *optimized*
  +- nvme6 fc traddr=nn-0x2013d039ea951c45:pn-0x2014d039ea951c45,host_traddr=nn-  0x200000109bdacc75:pn-0x100000109bdacc75 live *optimized*

----
--
.NVMe / TCP
--
[listing]
----
# nvme list-subsys /dev/nvme1n1
----
* 輸出範例： *

[listing, subs="+quotes"]
----

nvme-subsys1 -NQN=nqn.1992-08.com.netapp:
sn.983de7f4b39411ee871ed039ea954d18:subsystem.nvme_tcp_1         hostnqn=nqn.2014-08.org.nvmexpress:uuid:
4c4c4544-0035-5910-804b-c2c04f444d33
iopolicy=round-robin
\
+- nvme5 tcp traddr=192.168.166.7,trsvcid=4420,host_traddr=192.168.166.6,src_addr=192.168.166.6 *live*
+- nvme4 tcp traddr=192.168.166.8,trsvcid=4420,host_traddr=192.168.166.6,src_addr=192.168.166.6 *live*
+- nvme2 tcp traddr=192.168.167.7,trsvcid=4420,host_traddr=192.168.167.6,src_addr=192.168.167.6 *live*
+- nvme1 tcp traddr=192.168.167.8,trsvcid=4420,host_traddr=192.168.167.6,src_addr=192.168.167.6 *live*

----
--
====
. 驗證NetApp外掛程式是否顯示每ONTAP 個版本名稱空間裝置的正確值：
+
[role="tabbed-block"]
====
.欄位
--
[listing]
----
# nvme netapp ontapdevices -o column
----
* 輸出範例： *

[listing]
----
Device        Vserver   Namespace Path
----------------------- ------------------------------
/dev/nvme0n1 vs_tcp           /vol/vol1/ns1



NSID       UUID                                   Size
------------------------------------------------------------
1          6fcb8ea0-dc1e-4933-b798-8a62a626cb7f	21.47GB
----
--
.JSON
--
[listing]
----
# nvme netapp ontapdevices -o json
----
* 輸出範例 *

[listing]
----
{

"ONTAPdevices" : [
{

"Device" : "/dev/nvme1n1", "Vserver" : "linux_tcnvme_iscsi", "Namespace_Path" : "/vol/tcpnvme_1_0_0/tcpnvme_ns", "NSID" : 1,
"UUID" : "1a42c652-1450-4a29-886a-b4ccc23e637d", "Size" : "21.47GB",
"LBA_Data_Size" : 4096,
"Namespace_Size" : 5242880
},

]
}


----
--
====




== 已知問題

在 ONTAP 版本中、 RHEL 9.4 的 NVMe 主機組態沒有已知問題。
